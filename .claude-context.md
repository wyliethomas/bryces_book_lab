# Bryce's Book Lab - Claude Code Context

**Last Updated:** November 15, 2025 (Week 46)
**Project Status:** ‚úÖ v1.0.1 Delivered & Deployed Successfully
**Client:** Bryce
**Developer:** PXP (Wylie Thomas)

---

## Project Overview

Bryce's Book Lab is an AI-powered book writing assistant built as an Electron desktop application. It helps users organize notes, extract topics, generate chapter outlines, and write book chapters using AI (OpenAI GPT-4 or local Ollama models).

**Tech Stack:**
- Electron (desktop application framework)
- React + Vite (frontend)
- SQLite (better-sqlite3) for local database
- TipTap (rich text editor)
- OpenAI API or Ollama for AI features
- electron-builder for DMG packaging

---

## Current Status

**Version:** 1.0.1
**Deployment:** Successfully deployed to client's macOS system
**Last Major Work:** Saturday, November 15, 2025

### Completed Features
- ‚úÖ First-launch onboarding modal for LLM configuration
- ‚úÖ Dual LLM provider support (OpenAI + Ollama) with dynamic switching
- ‚úÖ Hot-reload AI configuration (no restart needed)
- ‚úÖ Database backup/restore system
- ‚úÖ Personalized branding ("Bryce's Book Lab")
- ‚úÖ Production macOS DMG installer
- ‚úÖ Post-delivery client support and troubleshooting

### Recent Critical Fixes
1. **LLM Provider Initialization Bug** (Saturday PM)
   - Issue: App was initializing OpenAI by default, ignoring user's Ollama selection
   - Fix: Added reinitializeAI() function, called after onboarding and settings changes
   - Files: `electron/ai-service.js`, `electron/main.js`, `src/components/Onboarding/OnboardingModal.jsx`

2. **Ollama Model Name Mismatch** (Saturday Evening)
   - Issue: Client's Ollama configuration failed with wrong model name
   - Root cause: Ollama Desktop app doesn't show model tags (e.g., `:latest`)
   - Fix: Live troubleshooting session using `ollama list` command
   - Resolution: Client updated model name in settings, app now working

---

## Key Architecture

### IPC Communication Flow
```
Renderer Process (React)
  ‚Üì window.electronAPI.*
Preload Script (electron/preload.cjs)
  ‚Üì ipcRenderer.invoke()
Main Process (electron/main.js)
  ‚Üì ipcMain.handle()
Services (AIService, Database)
```

### LLM Provider Abstraction
- `AIService.initialize()` - Detects provider from database, skips if not configured
- `AIService.callLLM()` - Unified method that routes to OpenAI or Ollama
- `AIService.reinitializeAI()` - Hot-reload configuration without app restart
- Settings stored in SQLite with AES-256-CBC encryption for API keys

### Database Schema
- `books` - Book metadata (title, description, author)
- `chapters` - Chapter content with ordering (book_id, title, content, order_index)
- `notes` - User notes (content)
- `topics` - Extracted topics (name)
- `note_topics` - Junction table linking notes to topics
- `settings` - Key-value config (encrypted for sensitive data)

---

## Critical Files Reference

### Frontend Components
- `src/App.jsx` - Main app, onboarding detection
- `src/components/Onboarding/OnboardingModal.jsx` - First-launch LLM setup (lines 39-40: reinitializeAI call)
- `src/components/Settings/SettingsView.jsx` - Provider switching, backup UI (lines 72-73: reinitializeAI call)
- `src/components/Layout/LeftSidebar.jsx` - App branding
- `src/components/ChapterView.jsx` - Chapter editing with TipTap
- `src/components/TopicExplorer.jsx` - Topic management and outline generation

### Electron Backend
- `electron/main.js` - IPC handlers, window management
  - Line ~150: `settings:reinitializeAI` handler
  - Line ~160: `backup:export` handler
  - Line ~180: `backup:import` handler
- `electron/preload.cjs` - Bridge between renderer and main process
  - Line 57: reinitializeAI exposed to renderer
  - Lines 61-64: backup functions exposed
- `electron/ai-service.js` - LLM provider abstraction
  - Lines 12-33: initialize() with null provider handling
  - Lines 83-105: callLLM() unified method
  - Lines 50-81: callOllama() HTTP API integration
- `electron/database.js` - SQLite operations with encryption
  - createTopic() method uses `error.code.startsWith('SQLITE_CONSTRAINT')` fix

### Build & Config
- `package.json` - App name: "bryces-book-lab", productName: "Bryce's Book Lab"
- `build-mac.sh` - Automated macOS DMG build script
- `BUILD_FOR_MAC.md` - Build documentation
- `.env` - Environment variables (OPENAI_API_KEY optional)

---

## Development Workflow

### Running Locally
```bash
cd book-writing-assistant
npm install
npm run electron:dev  # Starts Vite dev server + Electron
```

### Building for macOS
```bash
# Must be run on macOS system (tested on M1 MacBook)
./build-mac.sh
# Or manually:
npm run build          # Build Vite frontend
npm run electron:build # Create DMG with electron-builder
```

### Testing Clean Install
1. Delete `book-writing-assistant.db` from userData directory
2. Launch app - should show onboarding modal
3. Select LLM provider (OpenAI or Ollama)
4. Configure provider settings
5. Verify AI features work correctly

---

## Known Issues & Gotchas

### Ollama Configuration
- **Important:** Ollama Desktop app doesn't show full model tags
- **Solution:** Use `ollama list` in terminal to see actual model names
- **Example:** Model might be `llama3.2:latest` not just `llama3.2`
- **Common models:** llama3.2, llama3.1, mistral, qwen2.5

### Database Location
- **macOS:** `~/Library/Application Support/bryces-book-lab/book-writing-assistant.db`
- **Linux:** `~/.config/bryces-book-lab/book-writing-assistant.db`
- **Windows:** `%APPDATA%\bryces-book-lab\book-writing-assistant.db`

### API Key Encryption
- Stored with AES-256-CBC in database
- Uses system-generated encryption key
- Placeholder `‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢` shown in UI for existing keys

### Building DMG
- Cannot cross-compile macOS DMG from Linux
- Must build on actual macOS system with Xcode Command Line Tools
- App ID: `com.bryce.booklab`

---

## Recent Session Summary

### Saturday Morning (Nov 15, 2025)
1. Added onboarding modal for first-time LLM setup
2. Refactored AI service to support OpenAI + Ollama
3. Implemented provider switching in Settings
4. Added backup/restore functionality
5. Rebranded to "Bryce's Book Lab"
6. Added "Built by PXP" credit

### Saturday Afternoon
1. Built production DMG on macOS
2. Discovered critical bug: LLM provider not respecting user configuration
3. Fixed initialization logic to skip if provider not configured
4. Added reinitializeAI() mechanism for hot-reload
5. Rebuilt and tested clean DMG installation
6. Delivered v1.0.1 to client

### Saturday Evening
1. Live screenshare troubleshooting with client
2. Diagnosed Ollama model name mismatch issue
3. Used `ollama list` to identify correct model name
4. Client updated settings with proper model tag
5. Verified all AI features working correctly
6. Project successfully deployed and operational
7. Updated EOW report with troubleshooting details

---

## Next Steps (If Needed)

### Potential Future Enhancements
- User feedback from Bryce after initial usage period
- Collaboration features (multi-user support)
- Cloud sync for backup/restore
- Additional AI model providers (Anthropic Claude, Gemini)
- Performance optimization based on real-world usage
- Windows/Linux installers

### If Resuming Work
1. Read this context file
2. Review `EOW-Report-Week46-2025.txt` for complete history
3. Check Git status for any uncommitted changes
4. Test current build with `npm run electron:dev`
5. Verify both OpenAI and Ollama providers still work

---

## Contact & Resources

**Developer:** Wylie Thomas, PXP
**Email:** wylie@pxp200.com
**Website:** pxp.dev

**Documentation:**
- Electron Docs: https://www.electronjs.org/docs
- Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md
- OpenAI API: https://platform.openai.com/docs
- TipTap Editor: https://tiptap.dev/docs

**Debugging Tips:**
- Check console logs for LLM provider initialization (üîç emoji logs)
- Verify database settings with SQLite browser
- Test Ollama connectivity: `curl http://localhost:11434/api/tags`
- Check OpenAI API key validity on platform.openai.com

---

## File Structure Quick Reference

```
book-writing-assistant/
‚îú‚îÄ‚îÄ electron/
‚îÇ   ‚îú‚îÄ‚îÄ main.js              # Main process, IPC handlers
‚îÇ   ‚îú‚îÄ‚îÄ preload.cjs          # Context bridge
‚îÇ   ‚îú‚îÄ‚îÄ ai-service.js        # LLM provider abstraction
‚îÇ   ‚îú‚îÄ‚îÄ database.js          # SQLite operations
‚îÇ   ‚îî‚îÄ‚îÄ pdf-service.js       # PDF generation
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx              # Main app component
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Onboarding/OnboardingModal.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Settings/SettingsView.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChapterView.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TopicExplorer.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Layout/LeftSidebar.jsx
‚îÇ   ‚îî‚îÄ‚îÄ main.jsx             # React entry point
‚îú‚îÄ‚îÄ package.json             # Dependencies & build config
‚îú‚îÄ‚îÄ vite.config.js           # Vite bundler config
‚îú‚îÄ‚îÄ build-mac.sh             # macOS build script
‚îú‚îÄ‚îÄ BUILD_FOR_MAC.md         # Build documentation
‚îî‚îÄ‚îÄ .env                     # Environment variables (optional)
```

---

**End of Context File**
This file contains everything needed to resume work on Bryce's Book Lab.
